Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/05/12 00:38:42 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 237855@c2212.crane.hcc.unl.edu
17/05/12 00:38:42 INFO SignalUtils: Registered signal handler for TERM
17/05/12 00:38:42 INFO SignalUtils: Registered signal handler for HUP
17/05/12 00:38:42 INFO SignalUtils: Registered signal handler for INT
17/05/12 00:38:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/12 00:38:43 INFO SecurityManager: Changing view acls to: jdixon
17/05/12 00:38:43 INFO SecurityManager: Changing modify acls to: jdixon
17/05/12 00:38:43 INFO SecurityManager: Changing view acls groups to: 
17/05/12 00:38:43 INFO SecurityManager: Changing modify acls groups to: 
17/05/12 00:38:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jdixon); groups with view permissions: Set(); users  with modify permissions: Set(jdixon); groups with modify permissions: Set()
17/05/12 00:38:43 INFO TransportClientFactory: Successfully created connection to /10.138.22.2:39654 after 115 ms (0 ms spent in bootstraps)
17/05/12 00:38:44 INFO SecurityManager: Changing view acls to: jdixon
17/05/12 00:38:44 INFO SecurityManager: Changing modify acls to: jdixon
17/05/12 00:38:44 INFO SecurityManager: Changing view acls groups to: 
17/05/12 00:38:44 INFO SecurityManager: Changing modify acls groups to: 
17/05/12 00:38:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jdixon); groups with view permissions: Set(); users  with modify permissions: Set(jdixon); groups with modify permissions: Set()
17/05/12 00:38:44 INFO TransportClientFactory: Successfully created connection to /10.138.22.2:39654 after 1 ms (0 ms spent in bootstraps)
17/05/12 00:38:44 INFO DiskBlockManager: Created local directory at /lustre/work/swanson/jdixon/tmp/spark-b90f5d4b-4acb-4012-9c83-913a2a1e14cc/executor-cf14f9e8-f1b5-4222-9c04-45cc95169596/blockmgr-44f18930-fbed-4b4e-8da4-f527c7d20ab9
17/05/12 00:38:44 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
17/05/12 00:38:44 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.138.22.2:39654
17/05/12 00:38:44 INFO WorkerWatcher: Connecting to worker spark://Worker@10.138.22.12:33208
17/05/12 00:38:44 INFO TransportClientFactory: Successfully created connection to /10.138.22.12:33208 after 1 ms (0 ms spent in bootstraps)
17/05/12 00:38:44 INFO WorkerWatcher: Successfully connected to spark://Worker@10.138.22.12:33208
17/05/12 00:38:44 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/12 00:38:44 INFO Executor: Starting executor ID 0 on host 10.138.22.12
17/05/12 00:38:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40545.
17/05/12 00:38:45 INFO NettyBlockTransferService: Server created on 10.138.22.12:40545
17/05/12 00:38:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/12 00:38:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 10.138.22.12, 40545, None)
17/05/12 00:38:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 10.138.22.12, 40545, None)
17/05/12 00:38:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 10.138.22.12, 40545, None)
17/05/12 00:38:45 INFO CoarseGrainedExecutorBackend: Got assigned task 0
17/05/12 00:38:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/12 00:38:45 INFO Executor: Fetching spark://10.138.22.2:39654/files/es_scatter.py with timestamp 1494567520055
17/05/12 00:38:45 INFO TransportClientFactory: Successfully created connection to /10.138.22.2:39654 after 2 ms (0 ms spent in bootstraps)
17/05/12 00:38:45 INFO Utils: Fetching spark://10.138.22.2:39654/files/es_scatter.py to /lustre/work/swanson/jdixon/tmp/spark-b90f5d4b-4acb-4012-9c83-913a2a1e14cc/executor-cf14f9e8-f1b5-4222-9c04-45cc95169596/spark-d6e6957f-109f-4bf7-bed4-19ef4652f0e4/fetchFileTemp3029790704959250482.tmp
17/05/12 00:38:45 INFO Utils: Copying /lustre/work/swanson/jdixon/tmp/spark-b90f5d4b-4acb-4012-9c83-913a2a1e14cc/executor-cf14f9e8-f1b5-4222-9c04-45cc95169596/spark-d6e6957f-109f-4bf7-bed4-19ef4652f0e4/4227441341494567520055_cache to /lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/work/app-20170512003840-0000/0/./es_scatter.py
17/05/12 00:38:45 INFO Executor: Fetching spark://10.138.22.2:39654/jars/elasticsearch-hadoop-6.0.0.BUILD.SNAPSHOT.jar with timestamp 1494567519681
17/05/12 00:38:45 INFO Utils: Fetching spark://10.138.22.2:39654/jars/elasticsearch-hadoop-6.0.0.BUILD.SNAPSHOT.jar to /lustre/work/swanson/jdixon/tmp/spark-b90f5d4b-4acb-4012-9c83-913a2a1e14cc/executor-cf14f9e8-f1b5-4222-9c04-45cc95169596/spark-d6e6957f-109f-4bf7-bed4-19ef4652f0e4/fetchFileTemp284712086989509429.tmp
17/05/12 00:38:45 INFO Utils: Copying /lustre/work/swanson/jdixon/tmp/spark-b90f5d4b-4acb-4012-9c83-913a2a1e14cc/executor-cf14f9e8-f1b5-4222-9c04-45cc95169596/spark-d6e6957f-109f-4bf7-bed4-19ef4652f0e4/-15235995341494567519681_cache to /lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/work/app-20170512003840-0000/0/./elasticsearch-hadoop-6.0.0.BUILD.SNAPSHOT.jar
17/05/12 00:38:45 INFO Executor: Adding file:/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/work/app-20170512003840-0000/0/./elasticsearch-hadoop-6.0.0.BUILD.SNAPSHOT.jar to class loader
17/05/12 00:38:45 INFO TorrentBroadcast: Started reading broadcast variable 2
17/05/12 00:38:45 INFO TransportClientFactory: Successfully created connection to /10.138.22.2:34561 after 6 ms (0 ms spent in bootstraps)
17/05/12 00:38:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1789.0 B, free 912.3 MB)
17/05/12 00:38:45 INFO TorrentBroadcast: Reading broadcast variable 2 took 239 ms
17/05/12 00:38:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 912.3 MB)
17/05/12 00:38:46 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@30aaf81a
17/05/12 00:38:46 INFO TorrentBroadcast: Started reading broadcast variable 0
17/05/12 00:38:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 912.3 MB)
17/05/12 00:38:46 INFO TorrentBroadcast: Reading broadcast variable 0 took 24 ms
17/05/12 00:38:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.9 KB, free 912.0 MB)
17/05/12 00:38:46 INFO deprecation: mapred.mapoutput.value.class is deprecated. Instead, use mapreduce.map.output.value.class
17/05/12 00:38:47 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/05/12 00:38:47 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/05/12 00:38:47 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4589 bytes result sent to driver
17/05/12 00:38:50 INFO CoarseGrainedExecutorBackend: Got assigned task 1
17/05/12 00:38:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/05/12 00:38:50 INFO CoarseGrainedExecutorBackend: Got assigned task 2
17/05/12 00:38:50 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
17/05/12 00:38:50 INFO CoarseGrainedExecutorBackend: Got assigned task 3
17/05/12 00:38:50 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
17/05/12 00:38:50 INFO CoarseGrainedExecutorBackend: Got assigned task 4
17/05/12 00:38:50 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
17/05/12 00:38:50 INFO CoarseGrainedExecutorBackend: Got assigned task 5
17/05/12 00:38:50 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
17/05/12 00:38:50 INFO TorrentBroadcast: Started reading broadcast variable 3
17/05/12 00:38:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 912.0 MB)
17/05/12 00:38:50 INFO TorrentBroadcast: Reading broadcast variable 3 took 21 ms
17/05/12 00:38:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.5 KB, free 912.0 MB)
17/05/12 00:38:50 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@142f56dc
17/05/12 00:38:50 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@1100e891
17/05/12 00:38:50 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@52a4904f
17/05/12 00:38:50 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@5985d6e8
17/05/12 00:38:50 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@669b878c
17/05/12 00:38:50 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:50 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:50 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:50 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:50 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (org.apache.commons.httpclient.NoHttpResponseException) caught when processing request: The server 10.71.102.210 failed to respond
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3.368333333333334] for field [medianCpuBadput]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3997.84] for field [medianMemoryMB]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.JsonParserBase._handleEOF(JsonParserBase.java:473)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:2327)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:444)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [44.44444444444444] for field [maxWallClockHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWS(Utf8StreamParser.java:2303)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:502)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [44.44444444444444] for field [maxWallClockHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWS(Utf8StreamParser.java:2303)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:502)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_1 failed due to an exception
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3.368333333333334] for field [medianCpuBadput]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [28.62625] for field [meanCpuTimeHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [28.62625] for field [meanCpuTimeHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_2 failed due to an exception
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_3 failed due to an exception
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3997.84] for field [medianMemoryMB]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.JsonParserBase._handleEOF(JsonParserBase.java:473)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:2327)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:444)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_0 failed due to an exception
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_1 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_4 failed due to an exception
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_2 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_4 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_3 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 ERROR Executor: Exception in task 4.0 in stage 1.0 (TID 5)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [28.62625] for field [meanCpuTimeHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@1221e76f; line: 1, column: 4549301]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 4)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [44.44444444444444] for field [maxWallClockHr]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input within/between OBJECT entries
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@10a44aaa; line: 1, column: 1188009]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWS(Utf8StreamParser.java:2303)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:502)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3997.84] for field [medianMemoryMB]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1865095])
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@5fc35394; line: 1, column: 1868309]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.JsonParserBase._handleEOF(JsonParserBase.java:473)
	at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:2327)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:444)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 3)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: Cannot parse value [3.368333333333334] for field [medianCpuBadput]
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:705)
	at org.elasticsearch.hadoop.serialization.ScrollReader.map(ScrollReader.java:806)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:696)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHitAsMap(ScrollReader.java:466)
	at org.elasticsearch.hadoop.serialization.ScrollReader.readHit(ScrollReader.java:391)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:286)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:259)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:365)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
Caused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:95)
	at org.elasticsearch.hadoop.serialization.ScrollReader.parseValue(ScrollReader.java:720)
	at org.elasticsearch.hadoop.serialization.ScrollReader.read(ScrollReader.java:703)
	... 21 more
Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input in field name
 at [Source: org.apache.commons.httpclient.AutoCloseInputStream@6fe131b5; line: 1, column: 4183317]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
	at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
	at org.codehaus.jackson.impl.Utf8StreamParser.parseEscapedFieldName(Utf8StreamParser.java:1503)
	at org.codehaus.jackson.impl.Utf8StreamParser.slowParseFieldName(Utf8StreamParser.java:1404)
	at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:1231)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:495)
	at org.elasticsearch.hadoop.serialization.json.JacksonJsonParser.nextToken(JacksonJsonParser.java:93)
	... 23 more
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 6
17/05/12 00:38:57 INFO Executor: Running task 3.1 in stage 1.0 (TID 6)
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 7
17/05/12 00:38:57 INFO Executor: Running task 1.1 in stage 1.0 (TID 7)
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 8
17/05/12 00:38:57 INFO Executor: Running task 2.1 in stage 1.0 (TID 8)
17/05/12 00:38:57 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@2658e945
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 9
17/05/12 00:38:57 INFO Executor: Running task 0.1 in stage 1.0 (TID 9)
17/05/12 00:38:57 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@1cd21798
17/05/12 00:38:57 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@6157e4e7
17/05/12 00:38:57 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@1bfad699
17/05/12 00:38:57 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 10
17/05/12 00:38:57 INFO Executor: Running task 4.1 in stage 1.0 (TID 10)
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@65874ad0
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_3 failed due to an exception
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_3 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_0 failed due to an exception
17/05/12 00:38:57 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_1 failed due to an exception
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR Executor: Exception in task 3.1 in stage 1.0 (TID 6)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_1 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 9)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 WARN BlockManager: Putting block rdd_3_2 failed due to an exception
17/05/12 00:38:57 WARN BlockManager: Block rdd_3_2 could not be removed as it was not found on disk or in memory
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 11
17/05/12 00:38:57 ERROR Executor: Exception in task 1.1 in stage 1.0 (TID 7)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 INFO CoarseGrainedExecutorBackend: Got assigned task 12
17/05/12 00:38:57 INFO Executor: Running task 0.2 in stage 1.0 (TID 12)
17/05/12 00:38:57 ERROR Executor: Exception in task 2.1 in stage 1.0 (TID 8)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:57 INFO Executor: Running task 3.2 in stage 1.0 (TID 11)
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:57 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:57 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_4 failed due to an exception
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_4 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 ERROR Executor: Exception in task 4.1 in stage 1.0 (TID 10)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 13
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 14
17/05/12 00:38:58 INFO Executor: Running task 1.2 in stage 1.0 (TID 14)
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@6c0103d2
17/05/12 00:38:58 INFO Executor: Running task 2.2 in stage 1.0 (TID 13)
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@6a566914
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@2bdbab8b
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@78470ad8
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 15
17/05/12 00:38:58 INFO Executor: Running task 4.2 in stage 1.0 (TID 15)
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@5b96bcbd
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_0 failed due to an exception
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_2 failed due to an exception
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_1 failed due to an exception
17/05/12 00:38:58 ERROR Executor: Exception in task 0.2 in stage 1.0 (TID 12)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_1 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_2 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_3 failed due to an exception
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_3 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR Executor: Exception in task 1.2 in stage 1.0 (TID 14)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR Executor: Exception in task 2.2 in stage 1.0 (TID 13)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 16
17/05/12 00:38:58 ERROR Executor: Exception in task 3.2 in stage 1.0 (TID 11)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_4 failed due to an exception
17/05/12 00:38:58 INFO Executor: Running task 0.3 in stage 1.0 (TID 16)
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_4 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 17
17/05/12 00:38:58 INFO Executor: Running task 1.3 in stage 1.0 (TID 17)
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 18
17/05/12 00:38:58 INFO Executor: Running task 2.3 in stage 1.0 (TID 18)
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 19
17/05/12 00:38:58 INFO Executor: Running task 3.3 in stage 1.0 (TID 19)
17/05/12 00:38:58 ERROR Executor: Exception in task 4.2 in stage 1.0 (TID 15)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@2232fac7
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@21c7fd13
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@3699cfcc
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@67409450
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Got assigned task 20
17/05/12 00:38:58 INFO Executor: Running task 4.3 in stage 1.0 (TID 20)
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO NewHadoopRDD: Input split: org.elasticsearch.hadoop.mr.EsInputFormat$EsInputSplit@3b35331e
17/05/12 00:38:58 WARN EsInputFormat: Cannot determine task id...
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_0 failed due to an exception
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_0 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR Executor: Exception in task 0.3 in stage 1.0 (TID 16)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection refused (Connection refused)
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 INFO HttpMethodDirector: Retrying request
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_4 failed due to an exception
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_4 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 ERROR NetworkClient: Node [10.71.102.210:9200] failed (Connection refused (Connection refused)); no other nodes left - aborting...
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR Executor: Exception in task 4.3 in stage 1.0 (TID 20)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_1 failed due to an exception
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_1 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_2 failed due to an exception
17/05/12 00:38:58 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/worker.py", line 159, in main
    is_sql_udf = read_int(infile)
  File "/lustre/work/swanson/jdixon/CMS/CMS-Mining/spark-2/python/lib/pyspark.zip/pyspark/serializers.py", line 557, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:973)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR PythonRunner: This may have been caused by a prior exception:
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 WARN BlockManager: Putting block rdd_3_3 failed due to an exception
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_2 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 WARN BlockManager: Block rdd_3_3 could not be removed as it was not found on disk or in memory
17/05/12 00:38:58 ERROR Executor: Exception in task 1.3 in stage 1.0 (TID 17)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR Executor: Exception in task 2.3 in stage 1.0 (TID 18)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 ERROR Executor: Exception in task 3.3 in stage 1.0 (TID 19)
org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.71.102.210:9200]] 
	at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:150)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)
	at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:436)
	at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:363)
	at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:92)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.next(EsInputFormat.java:246)
	at org.elasticsearch.hadoop.mr.EsInputFormat$EsInputRecordReader.nextKeyValue(EsInputFormat.java:180)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:199)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)
	at org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)
17/05/12 00:38:58 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/12 00:38:58 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
